<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Posts - </title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&display=swap" rel="stylesheet">

    

    <link rel="stylesheet" href="/assets/css/style.css">
</head>
<body>
    <div class="container">
        <header>
            <nav>
                <a href="/" class="nav-name">Charlie O'Neill</a>
                <span class="nav-sep">|</span>
                <a href="/research/" >Research</a>
                <span class="nav-sep">|</span>
                <a href="/posts/" class="active">Posts</a>
            </nav>
        </header>

        <main>
            <h1>Posts</h1>

<div class="posts-list">


  <div class="year-header">2026</div>
  
    <div class="post-preview">
      <h2><a href="/posts/2026_books/">Books I read (am reading) in 2026</a></h2>
      
        <div class="post-description">I'm going to be better this year now I'm not a founder</div>
      
    </div>
  

  <div class="year-header">2025</div>
  
    <div class="post-preview">
      <h2><a href="/posts/anna/">Anna Karenina is the only honest answer to the terrible question</a></h2>
      
        <div class="post-description">From San Francisco</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/csuite/">Your MLEs are brilliant, but you’re giving them the wrong job</a></h2>
      
        <div class="post-description">A letter to the C-suite</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/bitterlessonevals/">The bitter lesson of LLM evals</a></h2>
      
        <div class="post-description">Scaling care beats scaling compute</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/alchemy/">We finally conquered alchemy</a></h2>
      
        <div class="post-description">But without economies of scale</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/aitimelines/">Five hinge questions that decide whether AGI is five years away or twenty</a></h2>
      
        <div class="post-description">For people who care about falsifiable stakes rather than vibes</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/deathtax/">Born on Third Base</a></h2>
      
        <div class="post-description">The Case for Inheriting Nothing and Building Everything</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/multidimfeat/">One-dimensional vs multi-dimensional features in interpretability</a></h2>
      
        <div class="post-description">How to stop conflating with 768 dimensions</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/korder/">LLMs are really good at k-order thinking (where k is even)</a></h2>
      
        <div class="post-description">You still need to tell a language model you want to cure cancer before it can help you cure cancer.</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/information-bounds-quantum-gravity/">Information bounds in quantum gravity</a></h2>
      
        <div class="post-description">How information theory links quantum mechanics and general relativity</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/2025_books/">Books I read (am reading) in 2025</a></h2>
      
        <div class="post-description">A dynamic list</div>
      
    </div>
  

  <div class="year-header">2024</div>
  
    <div class="post-preview">
      <h2><a href="/posts/neurips/">The nihilism of NeurIPS</a></h2>
      
        <div class="post-description">Some thoughts on our future in AI research</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/vqvae/">Can quantised autoencoders find and interpret circuits in language models?</a></h2>
      
        <div class="post-description">Using VQ-VAEs and categorical decision trees to do automatic circuit identification in LLMs.</div>
      
    </div>
  
    <div class="post-preview">
      <h2><a href="/posts/compression/">Learning compressed representations and GPT-5 speculation</a></h2>
      
        <div class="post-description">Why language models probably get too much from the abstraction we give them for free</div>
      
    </div>
  

</div>



        </main>

        <footer>
            <p>© 2026 Charlie O'Neill</p>
        </footer>
    </div>
</body>
</html>
